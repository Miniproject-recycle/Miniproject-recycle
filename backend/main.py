import sys
import os
from pathlib import Path

# AI ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
ai_model_path = Path(__file__).parent.parent / "ai_part" / "DetectionModel"
sys.path.append(str(ai_model_path))

from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.middleware.cors import CORSMiddleware
import shutil

# íƒì§€: ì´ë¯¸ì§€
from ai_part.DetectionModel.detect import detect, initialize_model

# VLM: ì´ë¯¸ì§€+ë¼ë²¨ / í…ìŠ¤íŠ¸ë§Œ ë‘ ê°€ì§€ ìœ í‹¸
from ai_part.vlm_model.vlm_service import (
    generate_recycling_guide,  # ì´ë¯¸ì§€ + ë¼ë²¨
    generate_recycling_guide_text_only,  # í…ìŠ¤íŠ¸ë§Œ
)

# ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¯¸ë¦¬ ë¡œë”©
print("ğŸš€ ì„œë²„ ì‹œì‘ - AI ëª¨ë¸ ë¡œë”© ì¤‘...")
initialize_model()
print("âœ… ì„œë²„ ì¤€ë¹„ ì™„ë£Œ!")

# from ai_part.VlmModel.vlm import run_vlm   # VLM ì²˜ë¦¬ í•¨ìˆ˜

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ë°°í¬ ì‹œ íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš© ê¶Œì¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì—…ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•  ì„ì‹œ ë””ë ‰í† ë¦¬
UPLOAD_DIR = "temp_uploads"
os.makedirs(UPLOAD_DIR, exist_ok=True)


# A) í…ìŠ¤íŠ¸ë§Œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° â†’ ë°”ë¡œ VLM í˜¸ì¶œ
@app.post("/vlm/guide-text")
async def vlm_guide_text(
    label: str = Form(..., description="í’ˆëª©/ë¼ë²¨ í…ìŠ¤íŠ¸ (ì˜ˆ: 'ì¢…ì´ë°•ìŠ¤')"),
):
    """
    í”„ë¡ íŠ¸ê°€ í…ìŠ¤íŠ¸ë§Œ ë³´ë‚¼ ë•Œ ì‚¬ìš©.
    - ë¶„ë¥˜/íƒì§€ ì—†ì´ ë¼ë²¨ í…ìŠ¤íŠ¸ë§Œìœ¼ë¡œ ë¶„ë¦¬ìˆ˜ê±° ê°€ì´ë“œ ìƒì„±.
    """

    try:
        result = generate_recycling_guide_text_only(label)
        if result.get("ok"):
            # í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì— ë§ê²Œ ì‘ë‹µ ë³€í™˜
            return {
                "ok": True,
                "label": label,
                "guide": result.get("guide_markdown"),  # guide_markdownì„ guideë¡œ ë³€í™˜
                "model": result.get("model"),
                "meta": result.get("meta")
            }
        else:
            # VLM ì—ëŸ¬ ì‹œ
            return {
                "ok": False,
                "label": label,
                "guide": None,
                "error": result.get("error"),
                "message": result.get("message")
            }
    except Exception as e:
        return {
            "ok": False,
            "label": label,
            "guide": None,
            "error": "GENERAL_ERROR",
            "message": f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
        }


# B) ì´ë¯¸ì§€ë§Œ ë“¤ì–´ì˜¤ëŠ” ê²½ìš° â†’ ê°ì²´ ê°ì§€ ì‹¤í–‰ & ë¼ë²¨ ë°˜í™˜ í•¨ìˆ˜
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        # ì—…ë¡œë“œëœ íŒŒì¼ ê²€ì¦
        if not file.content_type.startswith("image/"):
            raise HTTPException(
                status_code=400, detail="ì´ë¯¸ì§€ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            )

        # ì„ì‹œ íŒŒì¼ ì €ì¥
        file_path = os.path.join(UPLOAD_DIR, file.filename)
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ opt.sourceì— ì„¤ì •
        from ai_part.DetectionModel.detect import opt

        # ì ˆëŒ€ê²½ë¡œë¡œ ë³€í™˜ (ê²½ë¡œ ë¬¸ì œ í•´ê²°)
        absolute_file_path = os.path.abspath(file_path)
        opt.source = absolute_file_path

        print(f"AI ëª¨ë¸ì´ ì²˜ë¦¬í•  ì´ë¯¸ì§€ ê²½ë¡œ: {absolute_file_path}")
        print("ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ ì‹œì‘...")

        try:
            labels, scores = detect()  # ì‹¤ì œ AI ëª¨ë¸ í˜¸ì¶œ - ë¼ë²¨ê³¼ ìŠ¤ì½”ì–´ ë‘˜ ë‹¤ ë°›ê¸°
            print(f"ì‹¤ì œ AI ëª¨ë¸ ê²°ê³¼ - ë¼ë²¨: {labels}")
            print(f"ì‹¤ì œ AI ëª¨ë¸ ê²°ê³¼ - ìŠ¤ì½”ì–´: {scores}")
        except Exception as e:
            print(f"AI ëª¨ë¸ ì—ëŸ¬: {e}")
            import traceback

            traceback.print_exc()
            labels = ["unknown"]  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’
            scores = [0.0]  # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’

        # íŒŒì¼ ë°”ì´íŠ¸ë¡œ ì½ì–´ì„œ ì „ë‹¬
        with open(absolute_file_path, "rb") as f:
            image_bytes = f.read()

        # ë¼ë²¨ì´ ìˆìœ¼ë©´ VLM ì‹¤í–‰
        guide = None
        if labels and len(labels) > 0:
            try:
                guide_result = generate_recycling_guide(image_bytes, labels[0])
                if guide_result and guide_result.get("ok"):
                    guide = guide_result.get("guide_markdown")  # VLM ì„œë¹„ìŠ¤ëŠ” guide_markdownìœ¼ë¡œ ë°˜í™˜
            except Exception as vlm_error:
                print(f"VLM ì—ëŸ¬: {vlm_error}")
                guide = None

        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        os.remove(file_path)

        # í”„ë¡ íŠ¸ì—”ë“œ í˜•ì‹ì— ë§ê²Œ ì‘ë‹µ
        if labels:
            return {
                "ok": True,
                "label": labels[0],  # ì²« ë²ˆì§¸ ê°ì§€ëœ ê°ì²´
                "confidence": scores[0] if scores else 0.0,  # ì‹¤ì œ ì‹ ë¢°ë„ ìŠ¤ì½”ì–´
                "labels": labels,  # ì „ì²´ ê°ì§€ëœ ê°ì²´ë“¤
                "scores": scores,  # ì „ì²´ ì‹ ë¢°ë„ ìŠ¤ì½”ì–´ë“¤
                "total_detected": len(labels),
                "message": f"{len(labels)}ê°œì˜ ê°ì²´ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "guide": guide,
            }
        else:
            return {
                "ok": False,
                "label": "unknown",
                "confidence": 0.0,
                "labels": [],
                "message": "ë¶„ë¦¬ìˆ˜ê±° ëŒ€ìƒì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
                "guide": None
            }

    except Exception as e:
        print(f"âŒ AI ëª¨ë¸ ì—ëŸ¬ ë°œìƒ!")
        print(f"ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
        print(f"ì—ëŸ¬ ë©”ì‹œì§€: {str(e)}")

        # ì „ì²´ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
        import traceback

        print("=== ì „ì²´ ì—ëŸ¬ ìŠ¤íƒ ===")
        traceback.print_exc()
        print("===================")

        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        if os.path.exists(file_path):
            os.remove(file_path)

        # ì—ëŸ¬ ì‹œì—ë„ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
        return {
            "ok": False,
            "label": "error",
            "confidence": 0.0,
            "labels": [],
            "message": f"AI ëª¨ë¸ ì—ëŸ¬: {str(e)}",
            "guide": None
        }